{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIstributed Shared Nearest Neighbor experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if length(workers()) > 1\n",
    "    println(\"Removing previous workers...\")\n",
    "    rmprocs(workers())\n",
    "end\n",
    "\n",
    "nofworkers = 7\n",
    "addprocs(nofworkers)\n",
    "\n",
    "push!(LOAD_PATH, pwd())\n",
    "@everywhere using Distances\n",
    "@everywhere using StatsBase\n",
    "@everywhere using Clustering\n",
    "@everywhere using LightGraphs\n",
    "@everywhere include(\"WorkerSNN.jl\")\n",
    "@everywhere include(\"MasterSNN.jl\")\n",
    "@everywhere include(\"IOSNN.jl\")\n",
    "@everywhere include(\"SNNDBSCAN.jl\")\n",
    "@everywhere include(\"SNNGraphUtil.jl\")\n",
    "\n",
    "using PyCall\n",
    "@pyimport clustering_scores as cs #clustering_scores.py must be in the path.\n",
    "using JLD\n",
    "\n",
    "#DATA_PATH = \"./toy_example.dat\";\n",
    "#LABEL_PATH = \"./toy_example.dat.labels\";\n",
    "\n",
    "#DATA_PATH = \"./blobs.dat\";\n",
    "#LABEL_PATH = \"./blobs.dat.labels\";\n",
    "\n",
    "#DATA_PATH = \"./TDT2/tdt2_tfidf_top30.csv\";\n",
    "#LABEL_PATH = \"./TDT2/tdt2_tfidf_top30.csv.labels\";\n",
    "\n",
    "#DATA_PATH = \"./RCV1/reuters_single_tfidf_top30.csv\";\n",
    "#LABEL_PATH = \"./RCV1/reuters_single_tfidf_top30.csv.labels\";\n",
    "\n",
    "#DATA_PATH = \"./20newsgroups/20ng_tfidf_cai.csv\";\n",
    "#LABEL_PATH = \"./20newsgroups/20ng_tfidf_cai.csv.labels\";\n",
    "\n",
    "DATA_PATH = \"./20newsgroups/20ng_tfidf_cai_top10.csv\";\n",
    "LABEL_PATH = \"./20newsgroups/20ng_tfidf_cai_top10.csv.labels\";\n",
    "\n",
    "\n",
    "real_labels = vec(readdlm(LABEL_PATH, Int32));\n",
    "N, dim = get_header_from_input_file(DATA_PATH);\n",
    "\n",
    "#DATA = zeros(dim,N);\n",
    "#get_cluto_data(DATA, DATA_PATH);\n",
    "println(\"Dataset \",DATA_PATH,\" loaded (#Docs:\",N,\"/#Features:\",dim,\")\");\n",
    "pct_sample = 10; pct_sample = pct_sample/100; # (%) percentage of each local worker that will be sampled and transmitted to the Mas2ter\n",
    "\n",
    "#global score statistics (along cut_point values)\n",
    "summary_scores = Dict{String, Array{Tuple{Float64, Float64}, 1}}(\"elapsed\"=>[], \"bytesalloc\" => [], \"E\"=>[], \"P\" => [], \"ARI\" => [], \"AMI\" => [], \"NMI\" => [], \"H\" => [], \"C\" => [], \"VM\" => [])\n",
    "nruns = 10;# number of runs per cut_point value\n",
    "cut_values = collect(5:5:40);\n",
    "for cut_point=cut_values\n",
    "    @printf \"Starting runs with snn_cut_point:%d\\n\" cut_point \n",
    "    #score values attained along runs\n",
    "    run_scores = Dict{String, Array{Float64, 1}}(\"elapsed\"=>[], \"bytesalloc\" => [], \"E\"=>[], \"P\" => [], \"ARI\" => [], \"AMI\" => [], \"NMI\" => [], \"H\" => [], \"C\" => [], \"VM\" => [])\n",
    "\n",
    "    for run_no=collect(1:nruns)\n",
    "        partition = generate_partition(nofworkers, N); #N instances assigned to nofworkers cores.\n",
    "        # Performs the clustering task\n",
    "        results = Dict{String,Any}()        \n",
    "        _, elapsed_t, bytes_alloc, _, _ = @timed master_work(results, DATA_PATH, partition, pct_sample, similarity=\"cosine\", KNN=7, Eps_range=collect(5:5:40.0), MinPts_range=collect(5:5:40), k_range=[50], snn_cut_point=cut_point);\n",
    "\n",
    "        push!(run_scores[\"elapsed\"], elapsed_t)\n",
    "        push!(run_scores[\"bytesalloc\"], bytes_alloc)\n",
    "        \n",
    "        scores = cs.clustering_scores(real_labels, results[\"assignments\"], false);\n",
    "        scores = convert(Dict{String, Float64}, scores);\n",
    "        for qm=keys(scores)\n",
    "            push!(run_scores[qm], scores[qm])\n",
    "        end    \n",
    "    end\n",
    "    #compute mean and std for each score (along runs)\n",
    "    for qm=keys(run_scores)\n",
    "        qm_mean = mean(run_scores[qm])\n",
    "        qm_std = std(run_scores[qm])\n",
    "        push!(summary_scores[qm], (qm_mean, qm_std))\n",
    "    end \n",
    "end\n",
    "# save summary_scores\n",
    "jldoutput = join([DATA_PATH[1:end-4],\"_summary.jld\"]);#assumes that the data file ends with '.csv'\n",
    "JLD.save(jldoutput, \"summary_scores\", summary_scores)\n",
    "println(\"Storing summary to:\", jldoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Ensure that \\usepackage{pdflscape} is used.\n",
      "\n",
      "\\begin{landscape}\n",
      "\\begin{center}\n",
      "\\resizebox{\\columnwidth}{!}{%\n",
      "\\begin{tabular}{|c|c|c|c|c|c|}\n",
      "\\hline\n",
      "\\textbf{dataset}&\\textbf{cutpoint}&\\textbf{VM}&\\textbf{NMI}&\\textbf{AMI}&\\textbf{ARI}\\\\ \\hline\n",
      "\n",
      "KDD04(train) (#w:35) & 50 & 0.0245(0.0011)& 0.0388(0.0017)& 0.0131(0.0006)& 0.0019(0.0004)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 55 & 0.0247(0.0010)& 0.0390(0.0016)& 0.0132(0.0006)& 0.0020(0.0003)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 60 & 0.0247(0.0010)& 0.0391(0.0015)& 0.0132(0.0005)& 0.0018(0.0003)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 65 & 0.0245(0.0008)& 0.0387(0.0013)& 0.0131(0.0005)& 0.0020(0.0003)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 70 & 0.0248(0.0008)& 0.0394(0.0014)& 0.0133(0.0005)& 0.0020(0.0004)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 75 & 0.0248(0.0007)& 0.0393(0.0012)& 0.0132(0.0004)& 0.0020(0.0003)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 80 & 0.0248(0.0008)& 0.0393(0.0012)& 0.0133(0.0004)& 0.0021(0.0003)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 85 & 0.0242(0.0012)& 0.0383(0.0019)& 0.0129(0.0007)& 0.0019(0.0003)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 90 & 0.0247(0.0008)& 0.0392(0.0014)& 0.0132(0.0005)& 0.0020(0.0002)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 95 & 0.0251(0.0012)& 0.0397(0.0018)& 0.0134(0.0007)& 0.0021(0.0005)\\\\ \\hline\n",
      "KDD04(train) (#w:35) & 100 & 0.0244(0.0008)& 0.0386(0.0013)& 0.0130(0.0004)& 0.0019(0.0002)\\\\ \\hline\n",
      "\\end{tabular}}\n",
      "\\end{center}\n",
      "\\end{landscape}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition printable_run_score(String, Int64, Base.Dict{Any, Any}) in module Main at In[26]:2 overwritten at In[27]:2.\n",
      "WARNING: Method definition printable_run_score(String, Int64, Base.Dict{String, Tuple{Float64, Float64}}) in module Main at In[26]:24 overwritten at In[27]:24.\n",
      "WARNING: Method definition printable_run_score_summary(String, Array{Int64, 1}, Base.Dict{String, Array{Tuple{Float64, Float64}, 1}}) in module Main at In[26]:40 overwritten at In[27]:40.\n",
      "WARNING: Method definition printable_run_score_tex(String, Int64, Base.Dict{String, Tuple{Float64, Float64}}, Array{String, N<:Any}) in module Main at In[26]:67 overwritten at In[27]:67.\n",
      "WARNING: Method definition printable_run_score_summary_tex(String, Array{Int64, 1}, Base.Dict{String, Any}) in module Main at In[26]:97 overwritten at In[27]:97.\n",
      "WARNING: Method definition #printable_run_score_summary_tex(Array{Any, 1}, Main.#printable_run_score_summary_tex, String, Array{Int64, 1}, Base.Dict{String, Any}) in module Main overwritten.\n"
     ]
    }
   ],
   "source": [
    "function printable_run_score(dataset::String, cut_val::Int64, score_dict::Dict{Any, Any})\n",
    "    vm = score_dict[\"VM\"];\n",
    "    ari = score_dict[\"ARI\"];\n",
    "    nmi = score_dict[\"NMI\"];\n",
    "    ami = score_dict[\"AMI\"];\n",
    "    p = score_dict[\"P\"];\n",
    "    e = score_dict[\"E\"];\n",
    "    \n",
    "    output = \"\"\n",
    "    #output = \"|    dataset    | cut-point  |     VM     |     ARI    |     NMI    |     AMI    |      P     |      E     |\\n|---------------|------------|------------|------------|------------|------------|------------|------------|\\n\"\n",
    "    output = join([output,\n",
    "        @sprintf \"|  %s |  %d  | %.4f  |  %.4f  |  %.4f  |  %.4f  |  %.4f | %.4f  |\" dataset cut_val vm ari nmi ami p e])\n",
    "    return  output\n",
    "end\n",
    "\n",
    "#using PyCall\n",
    "#@pyimport clustering_scores as cs #clustering_scores.py must be in the path.\n",
    "#scores = cs.clustering_scores(real_labels, results[\"assignments\"], false);\n",
    "#println(printable_run_score(\"20NG\", 5, scores))\n",
    "\n",
    "\n",
    "function printable_run_score(dataset::String, cut_val::Int64, mean_std_dict::Dict{String, Tuple{Float64, Float64}})\n",
    "    # Generates a single row with the output in Markdown\n",
    "    measures = sort(collect(keys(mean_std_dict)))\n",
    "    output = @sprintf \"|  %s |  %d  |\" dataset cut_val\n",
    "    for qm=measures\n",
    "        mean_v, std_v = mean_std_dict[qm];\n",
    "        if qm == \"bytesalloc\"\n",
    "            mean_v = mean_v / (1024*1024)\n",
    "            std_v = std_v / (1024*1024)\n",
    "        end\n",
    "        current_cell = @sprintf \"  %.4f(%.4f)  |\" mean_v std_v;\n",
    "        output = join([output,current_cell])\n",
    "    end\n",
    "    return output\n",
    "end\n",
    "\n",
    "function printable_run_score_summary(dataset::String, cut_values::Array{Int64,1}, score_dict::Dict{String, Array{Tuple{Float64, Float64}, 1}})\n",
    "    # Generates the whole table in Markdown by calling the funciton above for each row\n",
    "    measures = sort(collect(keys(score_dict)));\n",
    "    output = \"|    dataset    | cut-point  |\"\n",
    "    n_cols = 2 + length(measures)\n",
    "    for qm=measures\n",
    "        output = join([output, @sprintf \"     %s   |\" qm])\n",
    "    end\n",
    "    output = join([output, \"\\n|\"])\n",
    "    #VM     |     ARI    |     NMI    |     AMI    |      P     |      E     |\n",
    "    for i=collect(1:n_cols)\n",
    "        output = join([output, \"---------------|\"])\n",
    "    end\n",
    "    \n",
    "    #one row per cut_value\n",
    "    for cut_ix=collect(1:length(cut_values))\n",
    "        mean_std_dict = Dict{String, Tuple{Float64, Float64}}()\n",
    "        for qm=measures\n",
    "            mean_std_dict[qm] = score_dict[qm][cut_ix]\n",
    "        end\n",
    "        output = join([output, \"\\n\", printable_run_score(dataset, cut_values[cut_ix], mean_std_dict)])\n",
    "    end\n",
    "    return output\n",
    "end\n",
    "\n",
    "\n",
    "function printable_run_score_tex(dataset::String, cut_val::Int64, mean_std_dict::Dict{String, Tuple{Float64, Float64}}, usedcolumns::Array{String})\n",
    "    # Generates a single row with the output in Markdown\n",
    "    #measures = sort(collect(keys(mean_std_dict)))\n",
    "    measures = usedcolumns;\n",
    "    output = @sprintf \"%s & %d \" dataset cut_val\n",
    "    for qm=measures\n",
    "        mean_v, std_v = mean_std_dict[qm];\n",
    "        if qm == \"bytesalloc\"\n",
    "            mean_v = mean_v / (1024*1024)\n",
    "            std_v = std_v / (1024*1024)\n",
    "        end\n",
    "        current_cell = @sprintf \"& %.4f(%.4f)\" mean_v std_v;\n",
    "        output = join([output,current_cell])\n",
    "    end\n",
    "    output = join([output,\"\\\\\\\\ \\\\hline\"])\n",
    "    return output\n",
    "end\n",
    "\n",
    "#=\n",
    "\\begin{center}\n",
    "  \\begin{tabular}{ | l | c | r }\n",
    "    \\hline\n",
    "    1 & 2 & 3 \\\\ \\hline\n",
    "    4 & 5 & 6 \\\\ \\hline\n",
    "    7 & 8 & 9 \\\\\n",
    "    \\hline\n",
    "  \\end{tabular}\n",
    "\\end{center}\n",
    "=#\n",
    "\n",
    "#function printable_run_score_summary_tex(dataset::String, cut_values::Array{Int64,1}, score_dict::Dict{String, Array{Tuple{Float64, Float64}, 1}};usedcolumns::Array{String}=String[])\n",
    "function printable_run_score_summary_tex(dataset::String, cut_values::Array{Int64,1}, score_dict::Dict{String, Any};usedcolumns::Array{String}=String[])\n",
    "    # Generates the whole table in Markdown by calling the funciton above for each row\n",
    "    measures = String[];\n",
    "    if length(usedcolumns) > 0\n",
    "        append!(measures, usedcolumns)\n",
    "    else\n",
    "        append!(measures, sort(collect(keys(score_dict))) );\n",
    "    end \n",
    "    n_cols = 2 + length(measures)\n",
    "    \n",
    "    output = \"\\% Ensure that \\\\usepackage{pdflscape} is used.\\n\\n\";\n",
    "    output = join([output,\"\\\\begin{landscape}\\n\\\\begin{center}\\n\\\\resizebox{\\\\columnwidth}{!}{%\\n\\\\begin{tabular}{|\"]);\n",
    "    for i=collect(1:n_cols)\n",
    "        output = join([output, \"c|\"])\n",
    "    end\n",
    "    output = join([output,\"}\\n\\\\hline\\n\"]);\n",
    "    \n",
    "    output = join([output,\"\\\\textbf{dataset}&\\\\textbf{cutpoint}\"])\n",
    "    for qm=measures\n",
    "        output = join([output, @sprintf \"&\\\\textbf{%s}\" qm])\n",
    "    end    \n",
    "    output = join([output, \"\\\\\\\\ \\\\hline\\n\"])\n",
    "    ##\n",
    "    #=\n",
    "    for i=collect(1:n_cols)\n",
    "        output = join([output, \"---------------|\"])\n",
    "    end\n",
    "    =#\n",
    "\n",
    "    #one row per cut_value\n",
    "    for cut_ix=collect(1:length(cut_values))\n",
    "        mean_std_dict = Dict{String, Tuple{Float64, Float64}}()\n",
    "        for qm=measures\n",
    "            mean_std_dict[qm] = score_dict[qm][cut_ix]\n",
    "        end\n",
    "        output = join([output, \"\\n\", printable_run_score_tex(dataset, cut_values[cut_ix], mean_std_dict, measures)])\n",
    "    end\n",
    "\n",
    "    ##\n",
    "    output = join([output, \"\\n\\\\end{tabular}}\\n\\\\end{center}\\n\\\\end{landscape}\"])\n",
    "    return output\n",
    "end\n",
    "\n",
    "\n",
    "using JLD\n",
    "cut_values = collect(5:5:40);\n",
    "includedcols = [\"VM\", \"NMI\", \"AMI\",\"ARI\"];\n",
    "#=\n",
    "summary = JLD.load(\"./20newsgroups/20ng_tfidf_cai_top10_summary.jld\")[\"summary_scores\"]\n",
    "println(printable_run_score_summary_tex(@sprintf(\"20Ng(top-10) (#w:%d)\",summary[\"nworkers\"]), summary[\"cut_range\"], summary, usedcolumns=includedcols))\n",
    "\n",
    "summary = JLD.load(\"./20newsgroups/20ng_tfidf_cai_summary.jld\")[\"summary_scores\"]\n",
    "println(printable_run_score_summary_tex(@sprintf(\"20Ng(all) (#w:%d)\",summary[\"nworkers\"]), summary[\"cut_range\"], summary, usedcolumns=includedcols))\n",
    "\n",
    "summary = JLD.load(\"./RCV1/reuters_single_tfidf_top30_summary.jld\")[\"summary_scores\"]\n",
    "println(printable_run_score_summary_tex(@sprintf(\"Reuters(top-30) (#w:%d)\",summary[\"nworkers\"]), summary[\"cut_range\"], summary, usedcolumns=includedcols))\n",
    "\n",
    "summary = JLD.load(\"./TDT2/tdt2_tfidf_top30_summary.jld\")[\"summary_scores\"]\n",
    "println(printable_run_score_summary_tex(@sprintf(\"TDT2(top-30) (#w:%d)\",summary[\"nworkers\"]), summary[\"cut_range\"], summary, usedcolumns=includedcols))\n",
    "=#\n",
    "summary = JLD.load(\"./KDD2004/phy_filtered_summary.jld\")[\"summary_scores\"]\n",
    "println(printable_run_score_summary_tex(@sprintf(\"KDD04(train) (#w:%d)\",summary[\"nworkers\"]), summary[\"cut_range\"], summary, usedcolumns=includedcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0,10.0,Inf)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[\"epsilon_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matshow(results[\"sampled_data_snn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a vector with pairwise snn-similarities \n",
    "L = Int64[]\n",
    "for i=collect(1:size(results[\"sampled_data_snn\"],1)-1)\n",
    "    for j=collect((i+1):size(results[\"sampled_data_snn\"],1))\n",
    "        push!(L, results[\"sampled_data_snn\"][i,j]);\n",
    "    end\n",
    "end\n",
    "\n",
    "test_hist = fit(Histogram, L);\n",
    "cpL = ecdf(L); # cumulative dist.\n",
    "\n",
    "fig = figure(\"pyplot_subplot_column\",figsize=(10,10));\n",
    "subplot(211);\n",
    "grid();\n",
    "title(\"PDF of pairwise snn-similarities\");\n",
    "bar(0:length(test_hist.weights)-1, test_hist.weights);\n",
    "xticks(0:length(test_hist.weights), test_hist.edges[1]);\n",
    "\n",
    "subplot(212);\n",
    "title(\"CDF of pairwise snn-similarities\");\n",
    "x = collect(1:test_hist.edges[1][end]);\n",
    "y = cpL(x);\n",
    "grid();\n",
    "bar(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralized experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"centralized_experiment.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./TDT2/tdt2_tfidf_top30.csv\";\n",
    "LABEL_PATH = \"./TDT2/tdt2_tfidf_top30.csv.labels\";\n",
    "println(\"TDT2 top30\")\n",
    "println(snn_grid_evaluation(DATA_PATH, LABEL_PATH))\n",
    "\n",
    "println(\"RCV1 top30\")\n",
    "DATA_PATH = \"./RCV1/reuters_single_tfidf_top30.csv\";\n",
    "LABEL_PATH = \"./RCV1/reuters_single_tfidf_top30.csv.labels\";\n",
    "println(snn_grid_evaluation(DATA_PATH, LABEL_PATH))\n",
    "\n",
    "println(\"20NG\")\n",
    "DATA_PATH = \"./20newsgroups/20ng_tfidf_cai.csv\";\n",
    "LABEL_PATH = \"./20newsgroups/20ng_tfidf_cai.csv.labels\";\n",
    "println(snn_grid_evaluation(DATA_PATH, LABEL_PATH))\n",
    "\n",
    "println(\"20NG top10\")\n",
    "DATA_PATH = \"./20newsgroups/20ng_tfidf_cai_top10.csv\";\n",
    "LABEL_PATH = \"./20newsgroups/20ng_tfidf_cai_top10.csv.labels\";\n",
    "println(snn_grid_evaluation(DATA_PATH, LABEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "__Centralized SNN Clustering results__\n",
    "```\n",
    "TDT2 top 30 (9394 x 36771) \n",
    "K       : 60\n",
    "epsilon : 25.0\n",
    "minpts  : 45\n",
    "VM      : 0.769667\n",
    "elapsed : 0.857351\n",
    "\n",
    "RCV1 top30 (8067 x 18933)\n",
    "K       : 80\n",
    "epsilon : 35.0\n",
    "minpts  : 65\n",
    "VM      : 0.482285\n",
    "elapsed : 0.967809\n",
    "\n",
    "20NG (18846 x 26214)\n",
    "K       : 30\n",
    "epsilon : 15.0\n",
    "minpts  : 15\n",
    "VM      : 0.300971\n",
    "elapsed : 3.92845\n",
    "\n",
    "20NG top10 (9917 x 26214)\n",
    "K       : 200\n",
    "epsilon : 75.0\n",
    "minpts  : 145\n",
    "VM      : 0.448386\n",
    "elapsed : 0.954286\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZONA DE PUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_points = size(DATA,2);\n",
    "Snn = zeros(Float64, num_points, num_points);\n",
    "S = zeros(Float64, num_points, num_points);\n",
    "shared_nn_sim(DATA, 110, Snn, S, similarity=\"cosine\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_point_cluster_id = Dict{Int64, Int64}();\n",
    "cluster_assignment = fill(SNNDBSCAN.UNCLASSIFIED, num_points);\n",
    "corepoints = Int64[];\n",
    "\n",
    "for i=collect(1:num_points)\n",
    "    d_point_cluster_id[i] = SNNDBSCAN.UNCLASSIFIED;\n",
    "end\n",
    "\n",
    "SNNDBSCAN.dbscan(num_points, 25.0, 30, Snn, d_point_cluster_id, corepoints)\n",
    "for i=collect(1:num_points)\n",
    "    cluster_assignment[i] = d_point_cluster_id[i]\n",
    "end\n",
    "using PyCall\n",
    "@pyimport clustering_scores as cs #clustering_scores.py must be in the path.\n",
    "scores = cs.clustering_scores(real_labels, cluster_assignment, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = tuned_snn_clustering(DATA, Eps_range = collect(3.0:15.0:40.0), MinPts_range = collect(10:10:30), k_range = [40, 90], similarity=\"cosine\")\n",
    "#assigned = find(x-> x>0, cluster_assignment)\n",
    "#mean(silhouettes(cluster_assignment[assigned], counts(cluster_assignment[assigned],maximum(cluster_assignment[assigned])), 110-Snn[assigned,assigned]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans results: Latex report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyCall\n",
    "@pyimport pickle\n",
    "includedcols = [\"VM\", \"NMI\", \"AMI\",\"P\",\"E\"];\n",
    "\n",
    "f=open(\"./20newsgroups/20ng_tfidf_cai_top10_kmeans_summary.pkl\",\"r\")\n",
    "km_summ = pickle.load(PyTextIO(f))\n",
    "kmscores = convert(Dict{String, Array{Tuple{Float64, Float64}, 1}}, km_summ)\n",
    "println(printable_run_score_summary_tex(\"20NG (top 10)\", collect(2:2:30), kmscores, usedcolumns=includedcols))\n",
    "close(f)\n",
    "\n",
    "f=open(\"./20newsgroups/20ng_tfidf_cai_kmeans_summary.pkl\",\"r\")\n",
    "km_summ = pickle.load(PyTextIO(f))\n",
    "kmscores = convert(Dict{String, Array{Tuple{Float64, Float64}, 1}}, km_summ)\n",
    "println(printable_run_score_summary_tex(\"20NG\", collect(2:2:30), kmscores, usedcolumns=includedcols))\n",
    "close(f)\n",
    "\n",
    "f=open(\"./RCV1/reuters_single_tfidf_top30_kmeans_summary.pkl\",\"r\")\n",
    "km_summ = pickle.load(PyTextIO(f))\n",
    "kmscores = convert(Dict{String, Array{Tuple{Float64, Float64}, 1}}, km_summ)\n",
    "println(printable_run_score_summary_tex(\"RCV1 (top 30)\", collect(2:2:30), kmscores, usedcolumns=includedcols))\n",
    "close(f)\n",
    "\n",
    "f=open(\"./TDT2/tdt2_tfidf_top30_kmeans_summary.pkl\",\"r\")\n",
    "km_summ = pickle.load(PyTextIO(f))\n",
    "kmscores = convert(Dict{String, Array{Tuple{Float64, Float64}, 1}}, km_summ)\n",
    "println(printable_run_score_summary_tex(\"TDT2 (top 30)\", collect(2:2:30), kmscores, usedcolumns=includedcols))\n",
    "close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Repeated Bisection results: Latex report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open(\"./20newsgroups/20ng_tfidf_cai_top10_kmeansRB_summary.pkl\",\"r\")\n",
    "km_summ = pickle.load(PyTextIO(f))\n",
    "kmscores = convert(Dict{String, Array{Tuple{Float64, Float64}, 1}}, km_summ)\n",
    "println(printable_run_score_summary_tex(\"20NG (top 10)\", collect(2:2:30), kmscores, usedcolumns=includedcols))\n",
    "close(f)\n",
    "\n",
    "f=open(\"./20newsgroups/20ng_tfidf_cai_kmeansRB_summary.pkl\",\"r\")\n",
    "km_summ = pickle.load(PyTextIO(f))\n",
    "kmscores = convert(Dict{String, Array{Tuple{Float64, Float64}, 1}}, km_summ)\n",
    "println(printable_run_score_summary_tex(\"20NG\", collect(2:2:30), kmscores, usedcolumns=includedcols))\n",
    "close(f)\n",
    "\n",
    "f=open(\"./RCV1/reuters_single_tfidf_top30_kmeansRB_summary.pkl\",\"r\")\n",
    "km_summ = pickle.load(PyTextIO(f))\n",
    "kmscores = convert(Dict{String, Array{Tuple{Float64, Float64}, 1}}, km_summ)\n",
    "println(printable_run_score_summary_tex(\"RCV1 (top 30)\", collect(2:2:30), kmscores, usedcolumns=includedcols))\n",
    "close(f)\n",
    "\n",
    "f=open(\"./TDT2/tdt2_tfidf_top30_kmeansRB_summary.pkl\",\"r\")\n",
    "km_summ = pickle.load(PyTextIO(f))\n",
    "kmscores = convert(Dict{String, Array{Tuple{Float64, Float64}, 1}}, km_summ)\n",
    "println(printable_run_score_summary_tex(\"TDT2 (top 30)\", collect(2:2:30), kmscores, usedcolumns=includedcols))\n",
    "close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"Elapsed time:\", elapsed_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, elapsed_t, bytes_alloc, _, _ = @timed 7^1000000000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments using the Cluto commands\n",
    "\n",
    "Este código puede ser usado para reemplazar su par en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DATA_PATH = \"./20newsgroups/20ng_tfidf_cai_top10.csv\"\n",
    "#LABEL_PATH = \"./20newsgroups/20ng_tfidf_cai_top10.csv.labels\"\n",
    "DATA_PATH = \"./KDD-CUP-99/corrected_numeric_sparse.csv\"\n",
    "LABEL_PATH = \"./KDD-CUP-99/corrected_numeric.csv.labels\"\n",
    "real_labels = vec(readdlm(LABEL_PATH, Int32));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function exec_cluto_rb(vectors_file::String, nclusters::Int64; CLUTOV_CMD::String=\"./cluto-2.1.2/Linux-x86_64/vcluster\"\n",
    ")\n",
    "    output = readstring(`$CLUTOV_CMD -clustfile=$vectors_file.k$nclusters $vectors_file $nclusters`);\n",
    "    \n",
    "    assign_fpath=@sprintf(\"%s.k%d\", vectors_file,nclusters)\n",
    "    \n",
    "    f = open(assign_fpath);\n",
    "    labels=Int64[];\n",
    "    for ln in eachline(f)\n",
    "        lbl_i=parse(Int64, ln);\n",
    "        push!(labels, lbl_i);\n",
    "    end\n",
    "    close(f)\n",
    "    return labels\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_assignment=exec_cluto_rb(DATA_PATH, 40);\n",
    "using PyCall\n",
    "@pyimport clustering_scores as cs #clustering_scores.py must be in the path.\n",
    "scores = cs.clustering_scores(real_labels, cluster_assignment, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Centralized SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include(\"IOSNN.jl\");\n",
    "include(\"WorkerSNN.jl\");\n",
    "include(\"SNNDBSCAN.jl\");\n",
    "using PyCall\n",
    "@pyimport clustering_scores as cs #clustering_scores.py must be in the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATA_PATH = \"./20newsgroups/20ng_tfidf_cai_top10.csv\";\n",
    "#LABEL_PATH = \"./20newsgroups/20ng_tfidf_cai_top10.csv.labels\";\n",
    "DATA_PATH = \"./cure_small.dat\";\n",
    "LABEL_PATH = \"./cure_small.dat.labels\";\n",
    "####\n",
    "real_labels = vec(readdlm(LABEL_PATH, Int64));\n",
    "num_points, dim = get_header_from_input_file(DATA_PATH);\n",
    "D = zeros(dim, num_points);\n",
    "get_cluto_data(D, DATA_PATH);\n",
    "####\n",
    "S = zeros(Float64, num_points, num_points);\n",
    "cosine_sim(D, S);\n",
    "Snn = zeros(Float64, num_points, num_points);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####\n",
    "shared_nn_sim(D, 150, Snn, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.098738 seconds (39.11 k allocations: 41.050 MB, 5.23% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 8 entries:\n",
       "  \"AMI\" => 0.557323\n",
       "  \"P\"   => 0.793665\n",
       "  \"C\"   => 0.560338\n",
       "  \"NMI\" => 0.609712\n",
       "  \"E\"   => 0.304178\n",
       "  \"VM\"  => 0.607544\n",
       "  \"H\"   => 0.663436\n",
       "  \"ARI\" => 0.465132"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eps, MinPts = 140, 10\n",
    "\n",
    "@time begin\n",
    "    cluster_assignment = SNNDBSCAN.snn_clustering(convert(Float64, Eps), MinPts, Snn)\n",
    "end\n",
    "scores = cs.clustering_scores(real_labels, cluster_assignment, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICVNN Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition cvnn_index(Array{Float64, 2}, Array{Int64, 1}) in module Main at /workspace/WorkerSNN.jl:7 overwritten at In[5]:3.\n",
      "WARNING: Method definition #cvnn_index(Array{Any, 1}, Main.#cvnn_index, Array{Float64, 2}, Array{Int64, 1}) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cvnn_index (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cvnn_index(D::Array{Float64,2}, labels::Array{Int64,1}; sep_k::Int64=10)\n",
    "    #computing separation\n",
    "    sep_score = 0.0\n",
    "    com_score = 0.0\n",
    "    for c=unique(labels)\n",
    "        points_in_c = find(x->x==c, labels)\n",
    "        n_c = size(points_in_c)[1]\n",
    "\n",
    "        sum_c = 0.0\n",
    "        for j=points_in_c\n",
    "            knn_j = sortperm(D[:,j])[2:(sep_k + 1)] #k-nst-n (ascending order in dist)\n",
    "            q_j = size(find(x->x!=c, labels[knn_j]))[1] #nst-n in different group\n",
    "            sum_c += q_j/sep_k\n",
    "        end\n",
    "        sep_c = (1.0/n_c)*sum_c #average weight for objs in the current cluster.\n",
    "        if sep_c > sep_score\n",
    "            sep_score = sep_c\n",
    "        end\n",
    "        ##\n",
    "        sum_c = 0.0\n",
    "        sims_c = D[points_in_c,points_in_c]\n",
    "        for i=collect(1:(n_c-1))\n",
    "            for j=collect((i+1):n_c)\n",
    "                sum_c += D[i,j]\n",
    "            end\n",
    "        end\n",
    "        com_score += (2.0/(n_c*(n_c-1)))*sum_c        \n",
    "    end\n",
    "    return (com_score + sep_score)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1481.9679429665641"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvnn_index(150-Snn, cluster_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933.2097563167848"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvnn_index(150-Snn, real_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.940735332432107"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvnn_index(1-S, cluster_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.484706011523611"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvnn_index(1-S, real_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition silhouette(Array{Float64, 2}, Array{Int64, 1}) in module Main at In[62]:9 overwritten at In[64]:9.\n",
      "WARNING: Method definition silhouette_i(Int64, Array{Float64, 2}, Array{Int64, 1}) in module Main at In[62]:24 overwritten at In[64]:24.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "silhouette_i (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function silhouette(D::Array{Float64,2}, labels::Array{Int64,1})\n",
    "    #D: Distance matrix\n",
    "    #labels: vector with assignments\n",
    "    #\n",
    "    # The best value is 1 and the worst value is -1. \n",
    "    # Values near 0 indicate overlapping clusters. \n",
    "    # Negative values generally indicate that a sample has been assigned to the wrong cluster\n",
    "    #\n",
    "    n = size(D)[1]\n",
    "    sil_sum = 0.0\n",
    "    for i=collect(1:n)\n",
    "        sil_sum += silhouette_i(i, D, labels)\n",
    "    end\n",
    "    return sil_sum / n\n",
    "end\n",
    "    \n",
    "function silhouette_i(i::Int64, D::Array{Float64,2}, labels::Array{Int64,1})    \n",
    "    #i: current point to examine\n",
    "    #D: Distance matrix\n",
    "    #labels: vector with assignments\n",
    "    #\n",
    "    # Compute the Silhouette Coefficient for a specific sample.\n",
    "    #\n",
    "    A = labels[i]\n",
    "    points_in_A = find(x->x==A, labels)\n",
    "    a_i = (sum(D[i, points_in_A])-D[i,i])/(size(points_in_A)[1] - 1)#It is assumed that D[i,i]:=0\n",
    "    b_i = Inf\n",
    "    for c=unique(labels) #computing min ave.dist among i and items in other clusters.\n",
    "        if c == A\n",
    "            continue\n",
    "        end\n",
    "        points_in_c = find(x->x==c, labels)\n",
    "        ave_dist_i = sum(D[i, points_in_c])/size(points_in_c)[1]\n",
    "        if ave_dist_i < b_i\n",
    "            b_i = ave_dist_i\n",
    "        end\n",
    "    end\n",
    "    return (b_i - a_i)/(max(b_i, a_i))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32684432851919676\n"
     ]
    }
   ],
   "source": [
    "println(silhouette(150-Snn, cluster_assignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3689959974079539"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Clustering\n",
    "assigned = find(x-> x>0, cluster_assignment);\n",
    "mean(Clustering.silhouettes(cluster_assignment[assigned], Clustering.counts(cluster_assignment[assigned],maximum(cluster_assignment[assigned])), 150-Snn[assigned,assigned]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_encoding (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function label_encoding(labels::Array{Int64})\n",
    "    new_label_map = Dict{Int64, Int64}();\n",
    "    p = 0;\n",
    "    for c=unique(labels)\n",
    "        p += 1;\n",
    "        new_label_map[c] = p;\n",
    "    end\n",
    "\n",
    "    new_labels = map((x) -> new_label_map[x], labels);\n",
    "    return new_labels\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3268443285191965"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(Clustering.silhouettes(new_labels, Clustering.counts(new_labels,maximum(new_labels)), 150-Snn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       "  1\n",
       "  6\n",
       " 11\n",
       " 16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(1:5:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
